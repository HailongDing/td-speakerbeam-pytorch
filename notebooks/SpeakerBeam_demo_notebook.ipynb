{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD-SpeakerBeam Demo Notebook\n",
    "\n",
    "This notebook demonstrates how to use the TD-SpeakerBeam model for target speech extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "from models.td_speakerbeam import TimeDomainSpeakerBeam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model\n",
    "\n",
    "Load a pre-trained TD-SpeakerBeam model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (replace with actual model path)\n",
    "model_path = '../example/model.pth'\n",
    "try:\n",
    "    model = TimeDomainSpeakerBeam.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Model file not found. Please train a model first or provide a valid model path.\")\n",
    "    # Create a dummy model for demonstration\n",
    "    model = TimeDomainSpeakerBeam(\n",
    "        i_adapt_layer=7,\n",
    "        adapt_layer_type='mul',\n",
    "        adapt_enroll_dim=128,\n",
    "        n_filters=512,\n",
    "        kernel_size=16,\n",
    "        stride=8\n",
    "    )\n",
    "    print(\"Created dummy model for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Files\n",
    "\n",
    "Load mixture and enrollment audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio files (replace with actual file paths)\n",
    "try:\n",
    "    mixture, sr = sf.read('../example/mixture.wav')\n",
    "    enrollment, _ = sf.read('../example/enrollment.wav')\n",
    "    print(f\"Loaded mixture: {mixture.shape}, enrollment: {enrollment.shape}\")\n",
    "    print(f\"Sample rate: {sr}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Audio files not found. Creating dummy signals.\")\n",
    "    sr = 8000\n",
    "    duration = 3.0\n",
    "    t = np.linspace(0, duration, int(sr * duration))\n",
    "    mixture = 0.5 * np.sin(2 * np.pi * 440 * t) + 0.3 * np.sin(2 * np.pi * 880 * t)\n",
    "    enrollment = 0.5 * np.sin(2 * np.pi * 440 * t)\n",
    "    print(f\"Created dummy signals: mixture: {mixture.shape}, enrollment: {enrollment.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Target Speech Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "mixture_tensor = torch.from_numpy(mixture).float().unsqueeze(0)\n",
    "enrollment_tensor = torch.from_numpy(enrollment).float().unsqueeze(0)\n",
    "\n",
    "# Perform extraction\n",
    "with torch.no_grad():\n",
    "    extracted = model(mixture_tensor, enrollment_tensor)\n",
    "    \n",
    "extracted_audio = extracted.squeeze().numpy()\n",
    "print(f\"Extracted audio shape: {extracted_audio.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot waveforms\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "time = np.arange(len(mixture)) / sr\n",
    "\n",
    "axes[0].plot(time, mixture)\n",
    "axes[0].set_title('Mixture')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "\n",
    "axes[1].plot(time[:len(enrollment)], enrollment)\n",
    "axes[1].set_title('Enrollment')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "\n",
    "axes[2].plot(time, extracted_audio)\n",
    "axes[2].set_title('Extracted Target Speech')\n",
    "axes[2].set_xlabel('Time (s)')\n",
    "axes[2].set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original mixture:\")\n",
    "display(Audio(mixture, rate=sr))\n",
    "\n",
    "print(\"Enrollment:\")\n",
    "display(Audio(enrollment, rate=sr))\n",
    "\n",
    "print(\"Extracted target speech:\")\n",
    "display(Audio(extracted_audio, rate=sr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}